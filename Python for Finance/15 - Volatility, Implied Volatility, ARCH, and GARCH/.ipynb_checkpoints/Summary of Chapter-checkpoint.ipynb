{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We typically assume that the volatility of returns for a security is a good measurement of risk\n",
    "    - The standard deviation or variance are both good measures of volatility\n",
    "        - The term \"volatility\" often refers to standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we model returns using a normal distribution, it will treat a positive deviation similarly to a negative deviation because the distribution is symmetrical\n",
    "    - This goes against our conventional wisdom; a negative deviation has a bigger impact than a positive deviation of the same amount\n",
    "        - To account for this asymmetry, the **Sortino measure** suggests a lower partial deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another assumption for the basic measurement of volatility is that it is constant over time\n",
    "    - This is not true; periods of high/low volatility are sticky but not constant\n",
    "        - To account for the inconsistency, we can use the **Auto Regressive Conditional Heteroskedasticity (ARCH)** process\n",
    "            - This process is expanded further using the **Generalized Auto Regressive Conditional Heteroskedasticity (GARCH)** process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# Conventional volatility measure - standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The term *volatility* usually refers to the standard deviation of returns for a security\n",
    "    - E.g. \"The volatility of IBM is 20%\" means the annualized standard deviation for the returns of IBM's stock was 0.2\n",
    "\n",
    "- Let's calculate the volatility for acutal IBM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_datareader import data as pdr\n",
    "import fix_yahoo_finance as yf\n",
    "yf.pdr_override()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded\n"
     ]
    }
   ],
   "source": [
    "df_IBM = pdr.get_data_yahoo('IBM', start = '2009-01-01', end='2014-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01-02</th>\n",
       "      <td>83.889999</td>\n",
       "      <td>87.589996</td>\n",
       "      <td>83.889999</td>\n",
       "      <td>87.370003</td>\n",
       "      <td>66.577576</td>\n",
       "      <td>7558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-05</th>\n",
       "      <td>86.419998</td>\n",
       "      <td>87.669998</td>\n",
       "      <td>86.180000</td>\n",
       "      <td>86.820000</td>\n",
       "      <td>66.158447</td>\n",
       "      <td>8315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-06</th>\n",
       "      <td>87.110001</td>\n",
       "      <td>90.410004</td>\n",
       "      <td>86.370003</td>\n",
       "      <td>89.230003</td>\n",
       "      <td>67.994942</td>\n",
       "      <td>9649500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-07</th>\n",
       "      <td>87.830002</td>\n",
       "      <td>88.800003</td>\n",
       "      <td>87.120003</td>\n",
       "      <td>87.790001</td>\n",
       "      <td>66.897629</td>\n",
       "      <td>8455100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-08</th>\n",
       "      <td>87.809998</td>\n",
       "      <td>88.139999</td>\n",
       "      <td>85.980003</td>\n",
       "      <td>87.180000</td>\n",
       "      <td>66.432793</td>\n",
       "      <td>7231800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close   Volume\n",
       "Date                                                                      \n",
       "2009-01-02  83.889999  87.589996  83.889999  87.370003  66.577576  7558200\n",
       "2009-01-05  86.419998  87.669998  86.180000  86.820000  66.158447  8315700\n",
       "2009-01-06  87.110001  90.410004  86.370003  89.230003  67.994942  9649500\n",
       "2009-01-07  87.830002  88.800003  87.120003  87.790001  66.897629  8455100\n",
       "2009-01-08  87.809998  88.139999  85.980003  87.180000  66.432793  7231800"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_IBM.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculating returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IBM['return'] = df_IBM['Adj Close'].shift(1)/df_IBM['Adj Close']-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside\n",
    "\n",
    "- Recall that if we have $n$ observations of daily returns, then the mean daily return is $\\frac{\\sum{r_{i}}}{n}$\n",
    "    - Similarly, the standard error of the mean is equal to $\\frac{\\sigma_{r}}{\\sqrt{n}}$\n",
    "\n",
    "- In this scenario, we can calculate the standard error of the mean by simply calculating the $\\sigma$ of our return column\n",
    "    - Therefore, to calculate the volatility of returns, we need to multiply the standard error by $\\sqrt{n}$\n",
    "    \n",
    "- We'll assume that there are 252 trading days in a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_error = np.std(df_IBM['return'])\n",
    "volatility = standard_error*np.sqrt(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20868237077512763"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, based on this data, we could say that the volatility of IBM over the 5-year period was about 21%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# Tests of normality\n",
    "\n",
    "## 1. Shapiro-Wilk test\n",
    "\n",
    "- We won't get into the details, but the Shapiro-Wilk test checks whether data is normally distributed\n",
    "    - Here, the null hypothesis is that the data IS normally distributed, so if the test rejects the null, the data is NOT normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9316559433937073, 1.5859177597029078e-23)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapiro(df_IBM['return'].dropna().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first value (0.9316...) is the test statistic, and the second (1.58...) is the p-value\n",
    "    - Since this p-value is well below the usual threshold of 0.05, we can **reject the null**\n",
    "        - Therefore, according to this test, the returns are NOT normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Anderson-Darling test\n",
    "\n",
    "- The scipy function for this test is similar to that of the Shapiro-Wilk test, except that we can alos use is for the exponential, logistic, and Gumbel distributions\n",
    "    - **Note**: the function's default test is for the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import anderson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AndersonResult(statistic=14.72247062396923, critical_values=array([0.574, 0.654, 0.785, 0.915, 1.089]), significance_level=array([15. , 10. ,  5. ,  2.5,  1. ]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anderson(df_IBM['return'].dropna().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This first value (14.72...) represents the test statistic\n",
    "\n",
    "- The next two represent the critical values and their corresponding levels of significance\n",
    "    - For example, if we want a 1% confidence interval, the test statistic must have a value of at least 1.089\n",
    "        - The value of 14.72 way exceeds this threshold, therefore we reject the null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# Estimating fat tails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall: the first 4 moments of a data set are:\n",
    "    1. Mean\n",
    "    2. Variance\n",
    "    3. Skew\n",
    "    4. Kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Mean: }\\mu = \\bar{r} = \\frac{\\sum{r_{i}}}{n} \\\\\n",
    "\\text{Variance: }\\sigma^{2} = \\frac{\\sum \\left (r_{i} - \\bar{r}\\right )^{2}}{n-1} \\\\\n",
    "\\text{Skew: }skew = \\frac{\\sum \\left (r_{i} - \\bar{r} \\right )^{3}}{(n-1)\\sigma^{3}} \\\\\n",
    "\\text{Kurtosis: } kurtosis = \\frac{\\sum\\left (r_{i} - \\bar{r} \\right )^{4}}{(n-1)\\sigma^{4}}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's estimate the first four moments of daily returns for the S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded\n"
     ]
    }
   ],
   "source": [
    "df_SP = pdr.get_data_yahoo('SPY', start = '1970-01-01', end = '2013-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SP['daily return'] = df_SP['Adj Close'].shift(1)/df_SP['Adj Close']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.00027222521985948664,\n",
       " 0.012187253117670008,\n",
       " 0.31700563261877923,\n",
       " 9.552432462334776)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_data = df_SP['daily return'].dropna().values\n",
    "mean = np.mean(ret_data)\n",
    "std = np.std(ret_data)\n",
    "skewness = skew(ret_data)\n",
    "kurtosis_ = kurtosis(ret_data)\n",
    "mean, std, skewness, kurtosis_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# Lower partial standard deviation and the Sortino ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One problem with using the simple standard deviation of daily returns is that a positive change is considered a bad thing\n",
    "    - Another problem is that we're comparing our performance to the mean, instead of a fixed benchmark (e.g. an assumed RFR of 5%)\n",
    "    \n",
    "- To account for these issues, Sortino suggested calculating the **lower partial standard deviation**:\n",
    "\n",
    "$$\n",
    "\\left (LPSD \\right ) = \\frac{\\sum \\left (r^{*}_{i} - RFR \\right )^{2}}{m-1}; \\text{ where }r^{*}_{i} \\leq RFR\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing the Fama-French RFR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RFR = pd.read_pickle('ffDaily.pkl')[['Rf']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Merging RFR onto the returns data for IBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IBM = df_IBM.join(df_RFR, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IBM['difference'] = df_IBM['return'] - df_IBM['Rf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>return</th>\n",
       "      <th>Rf</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01-02</th>\n",
       "      <td>83.889999</td>\n",
       "      <td>87.589996</td>\n",
       "      <td>83.889999</td>\n",
       "      <td>87.370003</td>\n",
       "      <td>66.577576</td>\n",
       "      <td>7558200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-05</th>\n",
       "      <td>86.419998</td>\n",
       "      <td>87.669998</td>\n",
       "      <td>86.180000</td>\n",
       "      <td>86.820000</td>\n",
       "      <td>66.158447</td>\n",
       "      <td>8315700</td>\n",
       "      <td>0.006335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-06</th>\n",
       "      <td>87.110001</td>\n",
       "      <td>90.410004</td>\n",
       "      <td>86.370003</td>\n",
       "      <td>89.230003</td>\n",
       "      <td>67.994942</td>\n",
       "      <td>9649500</td>\n",
       "      <td>-0.027009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.027009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-07</th>\n",
       "      <td>87.830002</td>\n",
       "      <td>88.800003</td>\n",
       "      <td>87.120003</td>\n",
       "      <td>87.790001</td>\n",
       "      <td>66.897629</td>\n",
       "      <td>8455100</td>\n",
       "      <td>0.016403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-08</th>\n",
       "      <td>87.809998</td>\n",
       "      <td>88.139999</td>\n",
       "      <td>85.980003</td>\n",
       "      <td>87.180000</td>\n",
       "      <td>66.432793</td>\n",
       "      <td>7231800</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close   Volume  \\\n",
       "Date                                                                         \n",
       "2009-01-02  83.889999  87.589996  83.889999  87.370003  66.577576  7558200   \n",
       "2009-01-05  86.419998  87.669998  86.180000  86.820000  66.158447  8315700   \n",
       "2009-01-06  87.110001  90.410004  86.370003  89.230003  67.994942  9649500   \n",
       "2009-01-07  87.830002  88.800003  87.120003  87.790001  66.897629  8455100   \n",
       "2009-01-08  87.809998  88.139999  85.980003  87.180000  66.432793  7231800   \n",
       "\n",
       "              return   Rf  difference  \n",
       "Date                                   \n",
       "2009-01-02       NaN  0.0         NaN  \n",
       "2009-01-05  0.006335  0.0    0.006335  \n",
       "2009-01-06 -0.027009  0.0   -0.027009  \n",
       "2009-01-07  0.016403  0.0    0.016403  \n",
       "2009-01-08  0.006997  0.0    0.006997  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_IBM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_data = df_IBM[df_IBM['difference']<0]['difference'].dropna().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculating the annualized LPSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1475022038478313"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(diff_data)*np.sqrt(252)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# Testing equivalency of volatility over two periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The stock market fell dramatically in Oct 1987\n",
    "    - We can compare market volatility before and after the drop using **Bartlett's test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can compare the performance of Ford Motor Corp before and after the crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded\n",
      "[*********************100%***********************]  1 of 1 downloaded\n"
     ]
    }
   ],
   "source": [
    "df_Ford_before = pdr.get_data_yahoo('F', start = '1982-09-01', end = '1987-09-01')\n",
    "df_Ford_after = pdr.get_data_yahoo('F', start = '1987-12-01', end = '1992-12-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Ford_before['return'] = df_Ford_before['Adj Close'].shift(1)/df_Ford_before['Adj Close']\n",
    "df_Ford_after['return'] = df_Ford_after['Adj Close'].shift(1)/df_Ford_after['Adj Close']\n",
    "\n",
    "ret_before = df_Ford_before['return'].dropna().values\n",
    "ret_after = df_Ford_after['return'].dropna().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.022901697327302302, 0.022991981703357433)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(ret_before), np.std(ret_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bartlett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartlettResult(statistic=0.019548491736464998, pvalue=0.8888054338011235)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bartlett(ret_before,ret_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For some reason, these results don't agree with those in the book. But whatever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# Testing heteroskedasticity\n",
    "\n",
    "- If volatility is constant, then residuals (i.e. noise) should be more or less random\n",
    "    - Otherwise, that means the errors increase/decrease with time (i.e. residuals are correlated to time)\n",
    "        - This is a problem in regression modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Skipping the rest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
