{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youtube playlist link: https://www.youtube.com/playlist?list=PLLK3oSbvdxFdF67yVxF_1FQO9SbBY3yTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is optimization?\n",
    "\n",
    "- this means choosing the right inputs to get the best possible outputs\n",
    "\n",
    "### Examples\n",
    "\n",
    "- what is the ideal launch angle to maximize the distance of a cannonball?\n",
    "- what's the best way to organize a warehouse to minimize transfer time?\n",
    "- what's the ideal way to build a bridge to maximize the load it can bear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- as we can already see from our example, most optimization problems are trying to **maximize** or **minimize** our outputs\n",
    "\n",
    "- often, there are also **constraints** on the inputs\n",
    "    - we only want to consider the possible combinations of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- usually, to solve optimization problems, we'll need to use an **optimization algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# Objective functions and decision variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "\n",
    "- the function we're trying to optimize\n",
    "\n",
    "### Example\n",
    "\n",
    "- if we wanted to make a square as big as possible, our objective function would be $f(x) = x^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sometimes we want to **minimize** our objective function\n",
    "    - sometimes we want to **maximize** our objective function\n",
    "        - sometimes we want our objective function to be as close as possible to some value\n",
    "            - **Note**: we can easily convert this to a minimization problem\n",
    "                - Using our square example above, if we wanted our square to have an area as close as possible to 50, we could redefine our objective function as $f(x) = x^{2} - 40$, and then we simply need to **minimize** it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- optimization problems are often written in the form:\n",
    "\n",
    "$$\n",
    "\\underset{x}{minimize}\\text{  } f(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision variables\n",
    "\n",
    "- these values are the inputs that our algorithm tweaks to try to find the best output\n",
    "\n",
    "- from our square example with objective function $f(x)$, the $x$ value is our decision varible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Note**: decision variables are sometimes called **design variables** or **manipulated variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# Gradients, constraints, and continuous/discrete/binary variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "- the gradient is the slope of the objective function\n",
    "\n",
    "- we can solve for the gradient using:\n",
    "    1. analytic differentiation\n",
    "    2. numerical differentiation\n",
    "    3. automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints\n",
    "\n",
    "- the contraints tell us what values the inputs are allowed to have\n",
    "\n",
    "### Example 1\n",
    "\n",
    "- suppose we need to enclose a rectangular field with a fence\n",
    "    - we have 500 feet of fencing material and a building is on one side of the field and so wonâ€™t need any fencing\n",
    "        - we want to determine the dimensions of the field that will enclose the largest area\n",
    "        \n",
    "- here, our constaint is an **equality constraint** since we need the sum of fence used to equal 500 ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "- suppose we're an uber driver and we want to make at least 5 uber trips before we go home for the night\n",
    "    - we want to find how we can minimize the amount of time we need to spend driving\n",
    "    \n",
    "- here, our constraint is an **inequality constraint**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- often, the optimal solution is very close to the constraint boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables: continuous vs. discrete vs. binary\n",
    "\n",
    "### Continuous variables examples\n",
    "\n",
    "- speed\n",
    "- weight\n",
    "- times\n",
    "\n",
    "### Discrete variables examples\n",
    "\n",
    "- the number of holes to be drilled in a board\n",
    "- the number of employees to hire\n",
    "\n",
    "- **Note**: discrete variables are often called integer variables\n",
    "\n",
    "\n",
    "### Binary variables examples\n",
    "\n",
    "- whether a switch is on or off\n",
    "- whether a person is a member of a group or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "\n",
    "# Gradient-based algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the two main types of methods for optimization problems are:\n",
    "    1. gradient-based\n",
    "    2. gradient-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gradient-based algorithms used derivatives to solve for the optimal solution\n",
    "    - this is the idea of trying to reach the base of a mountain as quickly as possible while blindfolded\n",
    "        - the best way is to always walk in the steepest direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the three main components of gradient-based algorithms are:\n",
    "    1. search direction\n",
    "    2. step size\n",
    "    3. convergence check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Search direction\n",
    "\n",
    "- this where we take the gradient, and use it to find the steepest direction we can go\n",
    "\n",
    "### 2. Step size\n",
    "\n",
    "- this tells us how far to go in our search direction before updating it\n",
    "    - the idea is that the steepest direction may change constantly\n",
    "    \n",
    "### 3. Convergence check\n",
    "\n",
    "- as we keep taking steps, we want to know how close we're getting to our target\n",
    "    - our convergence check tests whether we're close enough to consider the opitimization problem solved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "- our objective function is given by:\n",
    "\n",
    "$$\n",
    "f(x,y) = x^{3} + 15x^{2} + y^{3} + 15y^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the gradient of $f(x,y)$ is equal to:\n",
    "\n",
    "$$\n",
    "\\nabla f(x,y) = \\frac{\\partial f}{\\partial x}\\hat{x} + \\frac{\\partial f}{\\partial y}\\hat{y} = \\left (2x^{2} + 30x \\right )\\hat{x} + \\left (3y^{2}+ 30y \\right )\\hat{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for our initial condition, let's use $(x_{0},y_{0}) = (8,8)$\n",
    "    - at this starting point:\n",
    "    \n",
    "$$\n",
    "\\nabla f(8,8) = 432\\hat{x} + 432\\hat{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this means that if we go one **unit** step in the direction of $\\hat{x}$ and one in the direction of $\\hat{y}$, we'll increase our $f(x,y)$ value by 432+432=864\n",
    "    - so, regardless of  our step size, we should go in the opposite direction\n",
    "        - i.e. we should take a step from (8,8) towards (0,0) to descend as quickly as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and cons of gradient-based optimization\n",
    "\n",
    "### Pros\n",
    "\n",
    "1. widely used\n",
    "2. fast\n",
    "3. scales well to more variables\n",
    "\n",
    "### Cons\n",
    "\n",
    "1. requires smooth gradients\n",
    "    - if they're all over the place, they won't work\n",
    "2. need to continuously compute gradient\n",
    "3. can converge on a local minimum instead of the global minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# Gradient-free algorithms\n",
    "\n",
    "- as the name suggests, these algorithms don't require taking the derivative of the objective function\n",
    "    - this  is handy for problems where taking the derivative is not possible (e.g. **discrete decision variables**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gradient-free algorithms, however, are typically **much slower** than gradient-based ones\n",
    "    - they also may not converge on the same solution each time, and cannot guarantee the correct solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- some examples of gradient-free algorithms are:\n",
    "    1. exhaustive search\n",
    "    2. genetic algorithms\n",
    "    3. particle swarm\n",
    "    4. simulated annealing\n",
    "    5. nelder-mead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Exhaustive search\n",
    "\n",
    "- this is the simplest method, and also the most inefficient\n",
    "    - it involves simply checking every possible combination of input values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Genetic algorithms\n",
    "\n",
    "- this is where we test a subset of the possible combinations of input values\n",
    "    - we then rank the outcomes, and create a new subset of possible combinations around the highest ranked inputs\n",
    "        - this way, we focus in on the area around the best solution from each iteration until we converge on a final solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Particle swarm\n",
    "\n",
    "- this is similar to the genetic algorithm\n",
    "    - each solution is considered a particle and is assigned a direction and a velocity\n",
    "        - with each iteration, the new direction is a combination of i) its current direction, ii) the direction of its previous best score, and iii) the direction of the swarm's best score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Simulated annealing\n",
    "\n",
    "- this algorithm starts with calculating a score for an inital condition\n",
    "    - then an additional score is calculated for some random point close to the initial condition\n",
    "        - this process continues, but eventually, only if the score is better does a random point get selected in that direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Nelder-mead\n",
    "\n",
    "- this algorithm searches for the best point in a triangle around the inital condition\n",
    "    - the triangle then \"flip-flops\" in the direction of the best solution\n",
    "        - with each iteration, the triangle changes shape until it converges on a solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
